{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.context.SparkContext at 0x112f31bd0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd = sc.textFile(\"/Users/Enzo/Downloads/spark/README.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'# Apache Spark',\n",
       " u'',\n",
       " u'Spark is a fast and general cluster computing system for Big Data. It provides',\n",
       " u'high-level APIs in Scala, Java, Python, and R, and an optimized engine that',\n",
       " u'supports general computation graphs for data analysis. It also supports a',\n",
       " u'rich set of higher-level tools including Spark SQL for SQL and DataFrames,',\n",
       " u'MLlib for machine learning, GraphX for graph processing,',\n",
       " u'and Spark Streaming for stream processing.',\n",
       " u'',\n",
       " u'<http://spark.apache.org/>']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'when', 1),\n",
       " (u'alternatively,', 1),\n",
       " (u'\"local\"', 1),\n",
       " (u'including', 4),\n",
       " (u'computation', 1),\n",
       " (u'note', 1),\n",
       " (u'submit', 1),\n",
       " (u'using:', 1),\n",
       " (u'guidance', 2),\n",
       " (u'environment', 1)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# WORD COUNT problem \n",
    "# 1. Splits each word by each ' ' and puts it in a list\n",
    "# flatmap will gets rid off empty lists, and puts all elements in one list || also puts a 'u' next to each word\n",
    "# 2. map func makes everything lower case and assigns and attaches a 1 to it || ex: [..., (u'spark', 1), ...]\n",
    "# 3. reduceByKey func adds the previous element to the next element, it elemenets match it adds the key to it\n",
    "\n",
    "rdd.flatMap(lambda x: x.split()) \\\n",
    "    .map(lambda x: (x.lower(),1)) \\\n",
    "    .reduceByKey(lambda x,y: x+y) \\\n",
    "    .take(10)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'when', 1),\n",
       " (u'alternatively,', 1),\n",
       " (u'\"local\"', 1),\n",
       " (u'including', 4),\n",
       " (u'computation', 1),\n",
       " (u'note', 1),\n",
       " (u'submit', 1),\n",
       " (u'using:', 1),\n",
       " (u'guidance', 2),\n",
       " (u'environment', 1)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Another approach for WORD COUNT problem\n",
    "# 1. groupByKey func returns a pair (K,V) where V is an iterable. In this case it has the counts added up\n",
    "# 2. mapValues func sums the iterable from each pair\n",
    "\n",
    "rdd.flatMap(lambda x: x.split()) \\\n",
    "    .map(lambda x: (x.lower(),1)) \\\n",
    "    .groupByKey() \\\n",
    "    .mapValues(lambda x: sum(x)) \\\n",
    "    .take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# textFile() EXPLINATION\n",
    "# 1. minPartitions will be passed to Hadoop's InputFormat.getSplits. The parameter is a hint, so you may get more \n",
    "# or less partitions, depending on the Hadoop InputFormat implementation.\n",
    "# Logically split the set of input files for the job. Each InputSplit is then assigned to an individual \n",
    "# Mapper for processing\n",
    "\n",
    "chunks = sc.textFile(\"/Users/Enzo/Downloads/spark/README.md\", minPartitions=3) # Sets it to aleast 3. By default is 2\n",
    "chunks.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'when', 1), (u'alternatively,', 1)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Another approach for WORD COUNT problem \n",
    "# 1. wc is the chunks and each chunk gets applied mapPartitions and reduceBykey funcs\n",
    "# 2. mapPartitions func works for every partition (block) and is similar to regular map func\n",
    "# 1. k1v1 is a list of lists - holding the counts for each word\n",
    "# 2. at the end wc holds all the chunks together\n",
    "\n",
    "def mapper(k1v1s): \n",
    "    for k1v1 in k1v1s:\n",
    "        for word in k1v1.split():\n",
    "            yield (word.lower(),1)\n",
    "\n",
    "wc = chunks.mapPartitions(mapper).reduceByKey(lambda x, y: x+y)\n",
    "wc.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, [(u'the', 25), (u'to', 19), (u'spark', 16)])]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TOP 3 - WORD COUNT problem\n",
    "# 1. heapq is a priority queue\n",
    "# 2. nlargest(n, iterable[, key]) This function returns the n largest elements in the given iterable.\n",
    "# 3. mapper2 gets applied to each chunck in wc. Then yields the top3 words with the highest count. The key suggests\n",
    "# where to apply the between elements comparisons for the priority queue (heapq)\n",
    "# 4. reducer2 grabs the top 3 words with the highest count from each chunck and adds them up\n",
    "# 5. collect() - Return all the elements of the dataset as an array at the driver program. This is usually useful \n",
    "# after a filter or other operation that returns a sufficiently small subset of the data.\n",
    "\n",
    "import heapq \n",
    "\n",
    "def mapper2(counts):\n",
    "    yield (0, heapq.nlargest(3, counts, key=lambda x: x[1]))\n",
    "    \n",
    "def reducer2(top31,top32):\n",
    "    return heapq.nlargest(3, top31 + top32, key=lambda x: x[1])\n",
    "\n",
    "top3  = wc.mapPartitions(mapper2).reduceByKey(reducer2)\n",
    "top3.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'the', 25), (u'to', 19), (u'spark', 16)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Anotehr approach -- TOP 3 - WORD COUNT problem\n",
    "# !. reduce func is used here instead. It get rid off the starting 0 from the previous output\n",
    "\n",
    "import heapq\n",
    "\n",
    "def mapper2(counts):\n",
    "    yield heapq.nlargest(3, counts, key=lambda x: x[1])\n",
    "    \n",
    "def reducer2(top31,top32):\n",
    "    return heapq.nlargest(3, top31 + top32, key=lambda x: x[1])\n",
    "\n",
    "\n",
    "top3 = wc.mapPartitions(mapper2).reduce(reducer2)\n",
    "top3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# why cache? Spark will keep the elements around on the cluster for much faster access the next time you query it\n",
    "trips = sc.textFile(\"citibike.csv\").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['801', '379', '2474', '818']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CITIBIKE Problem\n",
    "# 1. mapPartitionWithIndex func -- Similar to mapPartitions, but also provides func with an integer value \n",
    "# representing the index of the partition\n",
    "\n",
    "\n",
    "def mapper3(partId, records):\n",
    "    if partId==0:\n",
    "        records.next()\n",
    "        \n",
    "    import csv\n",
    "    reader = csv.reader(records)\n",
    "    for trip in reader:\n",
    "        yield trip[2]\n",
    "\n",
    "output = trips.mapPartitionsWithIndex(mapper3)\n",
    "output.take(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
